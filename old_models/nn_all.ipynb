{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as keras_backend\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, LeakyReLU, SpatialDropout2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquivo de metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_path = 'spectogram/'\n",
    "#spectograms = os.listdir(audio_path)\n",
    "\n",
    "#metadata with most_freq chords indicator\n",
    "df = pd.read_csv('metadata.csv', sep = ',')\n",
    "df_chords = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"X_PCP_SOLO.npy\")\n",
    "y = np.load(\"y_PCP_SOLO.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2894, 12, 1216)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_test_encoded = to_categorical(le.fit_transform(y_test))\n",
    "y_train_encoded = to_categorical(le.fit_transform(y_train))\n",
    "\n",
    "\n",
    "mapping = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('chords_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 12, 1216)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/08 21:19:43 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.tensorflow.autolog(every_n_iter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(755, 1216, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reshape(755,1216,12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How data should be structured\n",
    "num_rows = round(1216/4)\n",
    "num_columns = 12*4 \n",
    "num_channels = 1\n",
    "\n",
    "# Reshape to fit the network input (channel last)\n",
    "X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "# Total number of labels to predict (equal to the network output nodes)\n",
    "num_labels = y_train_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    # Create a secquential object\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # Conv 1\n",
    "    model.add(Conv2D(filters=32, \n",
    "                     kernel_size=(3, 3), \n",
    "                     input_shape=(num_rows, num_columns, num_channels)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=32, \n",
    "                     kernel_size=(3, 3)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    # Max Pooling #1\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, \n",
    "                     kernel_size=(3, 3)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=64, \n",
    "                     kernel_size=(3,3)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "   \n",
    "    # Reduces each h√ów feature map to a single number by taking the average of all h,w values.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "    # Softmax output\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 302, 46, 32)       320       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 302, 46, 32)       0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 302, 46, 32)       128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 300, 44, 32)       9248      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 300, 44, 32)       0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300, 44, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 150, 22, 32)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 148, 20, 64)       18496     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 148, 20, 64)       0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 148, 20, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 146, 18, 64)       36928     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 146, 18, 64)       0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 146, 18, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 64)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 42)                2730      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68490 (267.54 KB)\n",
      "Trainable params: 68106 (266.04 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],  \n",
    "    optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 56s 2s/step - loss: 2.9668 - categorical_accuracy: 0.1424 - val_loss: 3.2443 - val_categorical_accuracy: 0.0940\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 56s 2s/step - loss: 2.9244 - categorical_accuracy: 0.1596 - val_loss: 3.0594 - val_categorical_accuracy: 0.1192\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 57s 2s/step - loss: 2.8883 - categorical_accuracy: 0.1596 - val_loss: 3.5743 - val_categorical_accuracy: 0.0842\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 58s 2s/step - loss: 2.8771 - categorical_accuracy: 0.1638 - val_loss: 3.8791 - val_categorical_accuracy: 0.0870\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 59s 2s/step - loss: 2.8626 - categorical_accuracy: 0.1700 - val_loss: 3.9840 - val_categorical_accuracy: 0.0715\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 61s 2s/step - loss: 2.8448 - categorical_accuracy: 0.1721 - val_loss: 4.3443 - val_categorical_accuracy: 0.0827\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 59s 2s/step - loss: 2.8317 - categorical_accuracy: 0.1721 - val_loss: 4.4536 - val_categorical_accuracy: 0.0743\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 59s 2s/step - loss: 2.8177 - categorical_accuracy: 0.1742 - val_loss: 4.4569 - val_categorical_accuracy: 0.0827\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 59s 2s/step - loss: 2.8131 - categorical_accuracy: 0.1818 - val_loss: 4.7883 - val_categorical_accuracy: 0.0757\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 59s 2s/step - loss: 2.7887 - categorical_accuracy: 0.1873 - val_loss: 4.8745 - val_categorical_accuracy: 0.0715\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 56s 2s/step - loss: 2.7842 - categorical_accuracy: 0.1852 - val_loss: 3.4849 - val_categorical_accuracy: 0.1010\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.7710 - categorical_accuracy: 0.1914 - val_loss: 4.4036 - val_categorical_accuracy: 0.0799\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.7571 - categorical_accuracy: 0.1942 - val_loss: 3.0137 - val_categorical_accuracy: 0.1290\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.7493 - categorical_accuracy: 0.1928 - val_loss: 4.2778 - val_categorical_accuracy: 0.0715\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.7309 - categorical_accuracy: 0.1963 - val_loss: 4.2514 - val_categorical_accuracy: 0.0701\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.7141 - categorical_accuracy: 0.1970 - val_loss: 4.2016 - val_categorical_accuracy: 0.0940\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6971 - categorical_accuracy: 0.1997 - val_loss: 3.3152 - val_categorical_accuracy: 0.1080\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6856 - categorical_accuracy: 0.2156 - val_loss: 3.7294 - val_categorical_accuracy: 0.1122\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6680 - categorical_accuracy: 0.2198 - val_loss: 3.1548 - val_categorical_accuracy: 0.1262\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6562 - categorical_accuracy: 0.2094 - val_loss: 3.0183 - val_categorical_accuracy: 0.1234\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6493 - categorical_accuracy: 0.2149 - val_loss: 4.5338 - val_categorical_accuracy: 0.0856\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6436 - categorical_accuracy: 0.2170 - val_loss: 3.4677 - val_categorical_accuracy: 0.0982\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6262 - categorical_accuracy: 0.2177 - val_loss: 3.7444 - val_categorical_accuracy: 0.0771\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.6110 - categorical_accuracy: 0.2122 - val_loss: 3.7563 - val_categorical_accuracy: 0.0701\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5896 - categorical_accuracy: 0.2377 - val_loss: 3.0739 - val_categorical_accuracy: 0.1431\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5785 - categorical_accuracy: 0.2225 - val_loss: 3.2510 - val_categorical_accuracy: 0.1360\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5475 - categorical_accuracy: 0.2281 - val_loss: 2.9207 - val_categorical_accuracy: 0.1529\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5416 - categorical_accuracy: 0.2294 - val_loss: 2.9785 - val_categorical_accuracy: 0.1403\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5270 - categorical_accuracy: 0.2446 - val_loss: 3.0756 - val_categorical_accuracy: 0.1318\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.5097 - categorical_accuracy: 0.2446 - val_loss: 2.8716 - val_categorical_accuracy: 0.1388\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 56s 2s/step - loss: 2.5018 - categorical_accuracy: 0.2502 - val_loss: 3.6584 - val_categorical_accuracy: 0.0856\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.4736 - categorical_accuracy: 0.2557 - val_loss: 3.2778 - val_categorical_accuracy: 0.1318\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 970s 35s/step - loss: 2.4587 - categorical_accuracy: 0.2543 - val_loss: 3.4058 - val_categorical_accuracy: 0.0926\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.4537 - categorical_accuracy: 0.2550 - val_loss: 2.8809 - val_categorical_accuracy: 0.1571\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 54s 2s/step - loss: 2.4315 - categorical_accuracy: 0.2674 - val_loss: 3.2624 - val_categorical_accuracy: 0.1332\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 57s 2s/step - loss: 2.4157 - categorical_accuracy: 0.2633 - val_loss: 3.0906 - val_categorical_accuracy: 0.1276\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.4046 - categorical_accuracy: 0.2744 - val_loss: 3.3287 - val_categorical_accuracy: 0.1178\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3855 - categorical_accuracy: 0.2757 - val_loss: 2.9687 - val_categorical_accuracy: 0.1753\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3765 - categorical_accuracy: 0.2730 - val_loss: 3.4096 - val_categorical_accuracy: 0.1417\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3460 - categorical_accuracy: 0.2889 - val_loss: 3.0323 - val_categorical_accuracy: 0.1599\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3383 - categorical_accuracy: 0.2723 - val_loss: 3.0572 - val_categorical_accuracy: 0.1164\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3078 - categorical_accuracy: 0.3013 - val_loss: 3.0497 - val_categorical_accuracy: 0.1206\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.2940 - categorical_accuracy: 0.3027 - val_loss: 2.7955 - val_categorical_accuracy: 0.1641\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3085 - categorical_accuracy: 0.3013 - val_loss: 5.4460 - val_categorical_accuracy: 0.0701\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 55s 2s/step - loss: 2.3002 - categorical_accuracy: 0.2951 - val_loss: 3.2499 - val_categorical_accuracy: 0.1332\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 56s 2s/step - loss: 2.2636 - categorical_accuracy: 0.2985 - val_loss: 3.1341 - val_categorical_accuracy: 0.1445\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 54s 2s/step - loss: 2.2386 - categorical_accuracy: 0.3193 - val_loss: 3.9886 - val_categorical_accuracy: 0.0870\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 54s 2s/step - loss: 2.2246 - categorical_accuracy: 0.3220 - val_loss: 3.6189 - val_categorical_accuracy: 0.0996\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 54s 2s/step - loss: 2.2062 - categorical_accuracy: 0.3110 - val_loss: 2.8915 - val_categorical_accuracy: 0.1529\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 54s 2s/step - loss: 2.1903 - categorical_accuracy: 0.3262 - val_loss: 2.6853 - val_categorical_accuracy: 0.1865\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train_encoded, \n",
    "                    batch_size=50, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_test,y_test_encoded),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar rede convolucional PCP\n",
    "X_train_ffn = X_train.reshape(2894,1216,12)\n",
    "X_test_ffn =  X_test.reshape(1426,1216,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "#mapping = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 3.0646 - accuracy: 0.2429\n",
      "Epoch 1: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 40ms/step - loss: 3.0646 - accuracy: 0.2429 - val_loss: 2.4826 - val_accuracy: 0.3366\n",
      "Epoch 2/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.9335 - accuracy: 0.4447\n",
      "Epoch 2: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 1.9335 - accuracy: 0.4447 - val_loss: 2.4179 - val_accuracy: 0.3689\n",
      "Epoch 3/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.6348 - accuracy: 0.5173\n",
      "Epoch 3: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 1.6348 - accuracy: 0.5173 - val_loss: 2.2609 - val_accuracy: 0.4236\n",
      "Epoch 4/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.6008 - accuracy: 0.5508\n",
      "Epoch 4: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 1.6008 - accuracy: 0.5508 - val_loss: 2.3139 - val_accuracy: 0.4060\n",
      "Epoch 5/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.2746 - accuracy: 0.5995\n",
      "Epoch 5: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 1.2746 - accuracy: 0.5995 - val_loss: 2.4115 - val_accuracy: 0.4313\n",
      "Epoch 6/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.1909 - accuracy: 0.6358\n",
      "Epoch 6: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 1.1909 - accuracy: 0.6358 - val_loss: 2.3460 - val_accuracy: 0.4425\n",
      "Epoch 7/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.1733 - accuracy: 0.6434\n",
      "Epoch 7: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 1.1733 - accuracy: 0.6434 - val_loss: 2.7407 - val_accuracy: 0.4053\n",
      "Epoch 8/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.1022 - accuracy: 0.6655\n",
      "Epoch 8: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 1.1022 - accuracy: 0.6655 - val_loss: 2.5231 - val_accuracy: 0.4404\n",
      "Epoch 9/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.9745 - accuracy: 0.6932\n",
      "Epoch 9: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 40ms/step - loss: 0.9745 - accuracy: 0.6932 - val_loss: 2.6490 - val_accuracy: 0.4095\n",
      "Epoch 10/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.9165 - accuracy: 0.7170\n",
      "Epoch 10: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.9170 - accuracy: 0.7167 - val_loss: 2.7598 - val_accuracy: 0.4348\n",
      "Epoch 11/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.8202 - accuracy: 0.7419\n",
      "Epoch 11: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.8202 - accuracy: 0.7419 - val_loss: 2.5283 - val_accuracy: 0.4649\n",
      "Epoch 12/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.7405\n",
      "Epoch 12: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.7993 - accuracy: 0.7405 - val_loss: 2.9611 - val_accuracy: 0.4348\n",
      "Epoch 13/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7873 - accuracy: 0.7569\n",
      "Epoch 13: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.7893 - accuracy: 0.7567 - val_loss: 2.8160 - val_accuracy: 0.4628\n",
      "Epoch 14/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.7548 - accuracy: 0.7636\n",
      "Epoch 14: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.7548 - accuracy: 0.7636 - val_loss: 3.1174 - val_accuracy: 0.4187\n",
      "Epoch 15/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.7263 - accuracy: 0.7585\n",
      "Epoch 15: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.7263 - accuracy: 0.7585 - val_loss: 2.8989 - val_accuracy: 0.4523\n",
      "Epoch 16/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5753 - accuracy: 0.8198\n",
      "Epoch 16: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.5747 - accuracy: 0.8196 - val_loss: 3.0111 - val_accuracy: 0.4537\n",
      "Epoch 17/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.8093\n",
      "Epoch 17: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.5780 - accuracy: 0.8093 - val_loss: 3.1062 - val_accuracy: 0.4411\n",
      "Epoch 18/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.8227\n",
      "Epoch 18: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 0.5396 - accuracy: 0.8227 - val_loss: 3.3408 - val_accuracy: 0.4376\n",
      "Epoch 19/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.8194\n",
      "Epoch 19: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 40ms/step - loss: 0.5662 - accuracy: 0.8189 - val_loss: 3.4077 - val_accuracy: 0.4446\n",
      "Epoch 20/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5721 - accuracy: 0.8160\n",
      "Epoch 20: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 0.5728 - accuracy: 0.8155 - val_loss: 3.6836 - val_accuracy: 0.4243\n",
      "Epoch 21/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4627 - accuracy: 0.8542\n",
      "Epoch 21: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 40ms/step - loss: 0.4627 - accuracy: 0.8542 - val_loss: 3.5756 - val_accuracy: 0.4313\n",
      "Epoch 22/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4738 - accuracy: 0.8483\n",
      "Epoch 22: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 0.4738 - accuracy: 0.8483 - val_loss: 3.6927 - val_accuracy: 0.4355\n",
      "Epoch 23/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.8476\n",
      "Epoch 23: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4675 - accuracy: 0.8476 - val_loss: 3.7997 - val_accuracy: 0.4313\n",
      "Epoch 24/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4460 - accuracy: 0.8559\n",
      "Epoch 24: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.4460 - accuracy: 0.8559 - val_loss: 4.0186 - val_accuracy: 0.4250\n",
      "Epoch 25/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.8601\n",
      "Epoch 25: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 35ms/step - loss: 0.4517 - accuracy: 0.8601 - val_loss: 4.0365 - val_accuracy: 0.4418\n",
      "Epoch 26/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.5269 - accuracy: 0.8323\n",
      "Epoch 26: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 35ms/step - loss: 0.5278 - accuracy: 0.8324 - val_loss: 4.1507 - val_accuracy: 0.4474\n",
      "Epoch 27/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.8597\n",
      "Epoch 27: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 0.4307 - accuracy: 0.8597 - val_loss: 3.8710 - val_accuracy: 0.4537\n",
      "Epoch 28/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8832\n",
      "Epoch 28: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 31ms/step - loss: 0.3353 - accuracy: 0.8832 - val_loss: 3.9139 - val_accuracy: 0.4432\n",
      "Epoch 29/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.8704\n",
      "Epoch 29: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 32ms/step - loss: 0.4047 - accuracy: 0.8704 - val_loss: 4.1614 - val_accuracy: 0.4530\n",
      "Epoch 30/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8773\n",
      "Epoch 30: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 35ms/step - loss: 0.4232 - accuracy: 0.8773 - val_loss: 4.0681 - val_accuracy: 0.4572\n",
      "Epoch 31/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9171\n",
      "Epoch 31: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.2429 - accuracy: 0.9171 - val_loss: 3.9548 - val_accuracy: 0.4727\n",
      "Epoch 32/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9229\n",
      "Epoch 32: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 35ms/step - loss: 0.2282 - accuracy: 0.9229 - val_loss: 4.0887 - val_accuracy: 0.4607\n",
      "Epoch 33/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9198\n",
      "Epoch 33: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 40ms/step - loss: 0.2659 - accuracy: 0.9195 - val_loss: 4.5130 - val_accuracy: 0.4383\n",
      "Epoch 34/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8840\n",
      "Epoch 34: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 41ms/step - loss: 0.3959 - accuracy: 0.8836 - val_loss: 4.4375 - val_accuracy: 0.4460\n",
      "Epoch 35/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8816\n",
      "Epoch 35: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.3605 - accuracy: 0.8818 - val_loss: 4.6470 - val_accuracy: 0.4544\n",
      "Epoch 36/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8525\n",
      "Epoch 36: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.4847 - accuracy: 0.8525 - val_loss: 5.0060 - val_accuracy: 0.4144\n",
      "Epoch 37/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.8338\n",
      "Epoch 37: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.6075 - accuracy: 0.8338 - val_loss: 5.3339 - val_accuracy: 0.4299\n",
      "Epoch 38/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.8728\n",
      "Epoch 38: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4876 - accuracy: 0.8728 - val_loss: 5.7619 - val_accuracy: 0.4271\n",
      "Epoch 39/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.6377 - accuracy: 0.8490\n",
      "Epoch 39: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.6377 - accuracy: 0.8490 - val_loss: 5.2332 - val_accuracy: 0.4257\n",
      "Epoch 40/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.8646\n",
      "Epoch 40: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4539 - accuracy: 0.8645 - val_loss: 5.4952 - val_accuracy: 0.4222\n",
      "Epoch 41/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9125\n",
      "Epoch 41: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.2824 - accuracy: 0.9129 - val_loss: 5.3076 - val_accuracy: 0.4201\n",
      "Epoch 42/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9354\n",
      "Epoch 42: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.1974 - accuracy: 0.9354 - val_loss: 4.9869 - val_accuracy: 0.4579\n",
      "Epoch 43/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9534\n",
      "Epoch 43: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 36ms/step - loss: 0.1405 - accuracy: 0.9534 - val_loss: 5.0398 - val_accuracy: 0.4719\n",
      "Epoch 44/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9580\n",
      "Epoch 44: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.1271 - accuracy: 0.9582 - val_loss: 5.1440 - val_accuracy: 0.4467\n",
      "Epoch 45/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9465\n",
      "Epoch 45: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 39ms/step - loss: 0.1665 - accuracy: 0.9461 - val_loss: 5.2261 - val_accuracy: 0.4376\n",
      "Epoch 46/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9378\n",
      "Epoch 46: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.1920 - accuracy: 0.9381 - val_loss: 5.3625 - val_accuracy: 0.4727\n",
      "Epoch 47/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9568\n",
      "Epoch 47: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.1625 - accuracy: 0.9568 - val_loss: 5.5243 - val_accuracy: 0.4544\n",
      "Epoch 48/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9395\n",
      "Epoch 48: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.1970 - accuracy: 0.9395 - val_loss: 5.1847 - val_accuracy: 0.4621\n",
      "Epoch 49/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9451\n",
      "Epoch 49: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 0.1656 - accuracy: 0.9451 - val_loss: 5.6922 - val_accuracy: 0.4383\n",
      "Epoch 50/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.8815\n",
      "Epoch 50: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.4444 - accuracy: 0.8815 - val_loss: 5.8447 - val_accuracy: 0.4432\n",
      "Epoch 51/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.7434 - accuracy: 0.8333\n",
      "Epoch 51: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.7463 - accuracy: 0.8324 - val_loss: 5.9777 - val_accuracy: 0.4313\n",
      "Epoch 52/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.8756\n",
      "Epoch 52: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4659 - accuracy: 0.8756 - val_loss: 6.1412 - val_accuracy: 0.4432\n",
      "Epoch 53/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8950\n",
      "Epoch 53: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 4s 39ms/step - loss: 0.3465 - accuracy: 0.8950 - val_loss: 6.2318 - val_accuracy: 0.4474\n",
      "Epoch 54/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8990\n",
      "Epoch 54: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4193 - accuracy: 0.8991 - val_loss: 6.5269 - val_accuracy: 0.4397\n",
      "Epoch 55/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.9541 - accuracy: 0.8552\n",
      "Epoch 55: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 38ms/step - loss: 0.9541 - accuracy: 0.8552 - val_loss: 6.3378 - val_accuracy: 0.4327\n",
      "Epoch 56/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8925\n",
      "Epoch 56: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4125 - accuracy: 0.8925 - val_loss: 6.5349 - val_accuracy: 0.4481\n",
      "Epoch 57/60\n",
      "90/91 [============================>.] - ETA: 0s - loss: 0.4962 - accuracy: 0.9122\n",
      "Epoch 57: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.4943 - accuracy: 0.9126 - val_loss: 6.2264 - val_accuracy: 0.4593\n",
      "Epoch 58/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9413\n",
      "Epoch 58: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.1942 - accuracy: 0.9413 - val_loss: 5.9427 - val_accuracy: 0.4509\n",
      "Epoch 59/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9585\n",
      "Epoch 59: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 37ms/step - loss: 0.1440 - accuracy: 0.9585 - val_loss: 6.1492 - val_accuracy: 0.4334\n",
      "Epoch 60/60\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9679\n",
      "Epoch 60: saving model to dashboard/model_checkpoint\\model.ckpt\n",
      "91/91 [==============================] - 3s 36ms/step - loss: 0.1110 - accuracy: 0.9679 - val_loss: 6.0276 - val_accuracy: 0.4677\n",
      "45/45 - 0s - loss: 6.0276 - accuracy: 0.4677 - 162ms/epoch - 4ms/step\n",
      "\n",
      "Test accuracy: 0.4677419364452362\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sigmoid(hidden_layer):\n",
    "    model_relu = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.Flatten(input_shape=(1216,12)), \n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.Dense(42,activation='softmax')\n",
    "  ]) \n",
    "\n",
    "#Compiling using loss function, Optimizer and Metrics\n",
    "    model_relu.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "    return model_relu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Feeding the model and Evalutating the Accuracy\n",
    "def model_fitting(model_fit):\n",
    "    checkpoint_path = \"dashboard/model_checkpoint/model.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    #teste como validacao pra ver o momento que erro come√ßou a subir\n",
    "    model_fit.fit(X_train_ffn, y_train_encoded, epochs=60, validation_data=(X_test_ffn,y_test_encoded), callbacks = [cp_callback])\n",
    "    test_loss, test_accuracy = model_fit.evaluate(X_test_ffn,  y_test_encoded, verbose=2)\n",
    "    print('\\nTest accuracy:', test_accuracy)\n",
    "    return test_accuracy, test_loss\n",
    "\n",
    "#Intializing the final array\n",
    "err = np.zeros(4)\n",
    "loss = np.zeros(4)\n",
    "\n",
    "#ReLU with 128 hidden values\n",
    "model = sigmoid(128)\n",
    "err[0],loss[0] = model_fitting(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Couldn't match files for checkpoint dashboard\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"dashboard/model.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1f0ee79f850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new model instance\n",
    "model = sigmoid(128)\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 - 0s - loss: 1.5765 - accuracy: 0.7251 - 132ms/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_ffn,  y_test_encoded, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ffn[0,:,:].reshape(1, 1216,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Acorde</th>\n",
       "      <th>Arquivo Original</th>\n",
       "      <th>most_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file_1393.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>A#:min</td>\n",
       "      <td>01_SS2-88-F_comp_hex.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file_1394.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>D#:7</td>\n",
       "      <td>01_SS2-88-F_comp_hex.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file_1395.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>G#:maj</td>\n",
       "      <td>01_SS2-88-F_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file_1396.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>C#:maj</td>\n",
       "      <td>01_SS2-88-F_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file_1397.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>G:hdim7</td>\n",
       "      <td>01_SS2-88-F_comp_hex.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>file_180806.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>F:min</td>\n",
       "      <td>00_Funk3-112-C#_comp_hex.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>file_180807.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>F#:maj</td>\n",
       "      <td>00_Funk3-112-C#_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>file_180808.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>C#:maj</td>\n",
       "      <td>00_Funk3-112-C#_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>file_180809.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>F#:maj</td>\n",
       "      <td>00_Funk3-112-C#_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>file_180810.wav</td>\n",
       "      <td>processed_audio/</td>\n",
       "      <td>G#:maj</td>\n",
       "      <td>00_Funk3-112-C#_comp_hex.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Filename               Loc   Acorde  \\\n",
       "0       file_1393.wav  processed_audio/   A#:min   \n",
       "1       file_1394.wav  processed_audio/     D#:7   \n",
       "2       file_1395.wav  processed_audio/   G#:maj   \n",
       "3       file_1396.wav  processed_audio/   C#:maj   \n",
       "4       file_1397.wav  processed_audio/  G:hdim7   \n",
       "...               ...               ...      ...   \n",
       "2155  file_180806.wav  processed_audio/    F:min   \n",
       "2156  file_180807.wav  processed_audio/   F#:maj   \n",
       "2157  file_180808.wav  processed_audio/   C#:maj   \n",
       "2158  file_180809.wav  processed_audio/   F#:maj   \n",
       "2159  file_180810.wav  processed_audio/   G#:maj   \n",
       "\n",
       "                  Arquivo Original  most_freq  \n",
       "0         01_SS2-88-F_comp_hex.wav          0  \n",
       "1         01_SS2-88-F_comp_hex.wav          0  \n",
       "2         01_SS2-88-F_comp_hex.wav          1  \n",
       "3         01_SS2-88-F_comp_hex.wav          1  \n",
       "4         01_SS2-88-F_comp_hex.wav          0  \n",
       "...                            ...        ...  \n",
       "2155  00_Funk3-112-C#_comp_hex.wav          0  \n",
       "2156  00_Funk3-112-C#_comp_hex.wav          1  \n",
       "2157  00_Funk3-112-C#_comp_hex.wav          1  \n",
       "2158  00_Funk3-112-C#_comp_hex.wav          1  \n",
       "2159  00_Funk3-112-C#_comp_hex.wav          1  \n",
       "\n",
       "[2160 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A#:maj'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.predict(X_test_ffn)[1, :]\n",
    "#X[0,:,:]\n",
    "\n",
    "#predi√ß√£o correta\n",
    "\n",
    "pred_aux = model.predict(X[0,:,:].reshape(1, 1216,12))\n",
    "\n",
    "nn.get_chord(pred_aux)\n",
    "\n",
    "#df_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dashboard.preprocessing_pipeline as pp \n",
    "\n",
    "#chroma = pp.PCP('dashboard/audio_teste.wav')[:, :1216]\n",
    "chroma2 =  pp.PCP('dashboard/chord_x.wav')[:, :1216]\n",
    "#padding = pp.add_padding(chroma2)\n",
    "# chroma = np.pad(chroma2, pad_width = ((0,0), (0,1216-chroma2.shape[1])))\n",
    "\n",
    "# padded_chroma = pp.add_padding(tf.expand_dims(chroma, 0))\n",
    "# expand_chroma = tf.expand_dims(padded_chroma[0], 1)\n",
    "# reshaped_chroma = tf.reshape(expand_chroma, [1, 1216, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 2397)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an numpy array of features, zero-pads each ocurrence to max_padding\n",
    "def add_padding(features, max_padding=174):\n",
    "    padded = []\n",
    "\n",
    "    # Add padding\n",
    "    for i in range(len(features)):\n",
    "        px = features[i]\n",
    "        size = len(px[0])\n",
    "        # Add padding if required\n",
    "        if (size < max_padding):\n",
    "            xDiff = max_padding - size\n",
    "            xLeft = xDiff//2\n",
    "            xRight = xDiff-xLeft\n",
    "            px = np.pad(px, pad_width=((0,0), (xLeft, xRight)), mode='constant')\n",
    "        \n",
    "        padded.append(px)\n",
    "\n",
    "    return padded\n",
    "\n",
    "padded  = add_padding([chroma2], max_padding= 1216)\n",
    "reshaped_padded = padded[0].reshape(1,1216,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:hdim7'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pipeline = model.predict(reshaped_padded)\n",
    "\n",
    "import dashboard.nn_amodel as nn \n",
    "\n",
    "\n",
    "nn.get_chord(pred_pipeline)\n",
    "\n",
    "#reshaped_chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x2abaedafb50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "librosa.display.specshow(chroma, y_axis='chroma', x_axis = 'time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37333637, 0.48328066, 0.48328066, ..., 0.07839249, 0.07940821,\n",
       "        0.07940821],\n",
       "       [0.42284214, 0.48739442, 0.4570775 , ..., 0.05244875, 0.05269052,\n",
       "        0.05269052],\n",
       "       [0.53522927, 0.54242903, 0.53522927, ..., 0.02593376, 0.02611391,\n",
       "        0.02623735],\n",
       "       ...,\n",
       "       [0.6381211 , 0.6381211 , 0.6381211 , ..., 0.07635138, 0.07727032,\n",
       "        0.07727032],\n",
       "       [0.54326725, 0.54326725, 0.54326725, ..., 0.09062101, 0.09167526,\n",
       "        0.09167526],\n",
       "       [0.47837007, 0.47837007, 0.47837007, ..., 0.08819962, 0.08965237,\n",
       "        0.08965237]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 4ms/step\n",
      "F1-Score:  0.6428898093241839\n",
      "F1-Score:  0.6428898093241839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        A#:7       0.00      0.00      0.00         3\n",
      "    A#:hdim7       0.33      0.67      0.44         3\n",
      "      A#:maj       0.65      0.81      0.72        32\n",
      "      A#:min       0.93      0.76      0.84        17\n",
      "         A:7       0.67      1.00      0.80         4\n",
      "     A:hdim7       1.00      0.33      0.50         3\n",
      "       A:maj       0.73      0.87      0.79        31\n",
      "       A:min       0.62      0.73      0.67        11\n",
      "       B:maj       0.56      0.69      0.62        26\n",
      "       B:min       0.56      0.36      0.43        14\n",
      "        C#:7       0.12      0.11      0.12         9\n",
      "    C#:hdim7       1.00      1.00      1.00         3\n",
      "      C#:maj       0.88      0.82      0.85        45\n",
      "      C#:min       0.80      0.80      0.80        15\n",
      "         C:7       0.92      0.80      0.86        15\n",
      "     C:hdim7       0.80      0.67      0.73         6\n",
      "       C:maj       0.88      0.80      0.83        44\n",
      "       C:min       0.50      0.22      0.31         9\n",
      "        D#:7       0.82      0.78      0.80        18\n",
      "      D#:maj       0.80      0.80      0.80        40\n",
      "      D#:min       0.75      0.50      0.60         6\n",
      "         D:7       0.00      0.00      0.00         3\n",
      "       D:maj       0.66      0.72      0.69        29\n",
      "       D:min       1.00      0.90      0.95        10\n",
      "         E:7       0.33      0.33      0.33         6\n",
      "     E:hdim7       0.80      1.00      0.89         4\n",
      "       E:maj       0.79      0.79      0.79        33\n",
      "       E:min       0.62      0.70      0.65        23\n",
      "        F#:7       0.73      0.89      0.80         9\n",
      "      F#:maj       0.72      0.82      0.77        34\n",
      "      F#:min       1.00      0.44      0.62         9\n",
      "         F:7       0.67      0.50      0.57         4\n",
      "     F:hdim7       0.40      0.40      0.40         5\n",
      "       F:maj       0.75      0.89      0.82        45\n",
      "       F:min       1.00      0.79      0.88        14\n",
      "        G#:7       1.00      0.50      0.67         2\n",
      "    G#:hdim7       0.00      0.00      0.00         2\n",
      "      G#:maj       0.81      0.86      0.83        35\n",
      "      G#:min       0.75      0.71      0.73        21\n",
      "     G:hdim7       0.75      0.50      0.60         6\n",
      "       G:maj       0.76      0.70      0.73        46\n",
      "       G:min       0.82      0.74      0.78        19\n",
      "\n",
      "    accuracy                           0.74       713\n",
      "   macro avg       0.68      0.64      0.64       713\n",
      "weighted avg       0.75      0.74      0.74       713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_score = model.predict(X_test)\n",
    "pred_idx = tf.math.argmax(preds_score, axis = 1)\n",
    "#y_test_idx = tf.math.argmax(y_test_encoded, axis = 1)\n",
    "target_names = mapping.keys()\n",
    "\n",
    "print('F1-Score: ', f1_score(y_test_encoded, pred_idx, average = 'macro'))\n",
    "print('F1-Score: ', f1_score(y_test_encoded, pred_idx, average = 'macro'))\n",
    "\n",
    "print(classification_report(y_test_encoded, pred_idx, target_names= target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(373,), dtype=int64, numpy=\n",
       "array([4, 1, 2, 1, 0, 7, 1, 3, 6, 4, 6, 4, 6, 7, 5, 8, 5, 8, 3, 8, 5, 0,\n",
       "       0, 7, 7, 1, 3, 3, 7, 2, 5, 7, 3, 2, 9, 5, 4, 4, 3, 4, 3, 9, 0, 6,\n",
       "       5, 8, 9, 1, 3, 4, 7, 8, 8, 1, 5, 4, 7, 1, 7, 7, 4, 9, 9, 0, 9, 7,\n",
       "       8, 9, 9, 3, 1, 1, 7, 9, 0, 7, 7, 1, 8, 2, 8, 9, 7, 5, 1, 0, 7, 5,\n",
       "       1, 1, 5, 4, 2, 8, 0, 7, 9, 5, 1, 6, 9, 5, 8, 9, 4, 8, 2, 6, 9, 0,\n",
       "       9, 2, 5, 4, 7, 7, 3, 6, 0, 9, 6, 5, 6, 3, 1, 5, 3, 1, 2, 4, 7, 5,\n",
       "       9, 2, 8, 2, 4, 6, 2, 6, 9, 4, 3, 1, 4, 9, 1, 4, 8, 9, 5, 6, 6, 7,\n",
       "       7, 1, 3, 7, 3, 8, 2, 7, 1, 6, 5, 9, 4, 2, 8, 6, 9, 2, 2, 6, 0, 5,\n",
       "       2, 8, 0, 8, 8, 0, 5, 0, 7, 4, 7, 5, 1, 9, 2, 9, 7, 8, 0, 4, 7, 3,\n",
       "       3, 7, 0, 9, 6, 2, 9, 3, 0, 9, 9, 6, 4, 1, 3, 2, 7, 4, 4, 9, 8, 6,\n",
       "       1, 7, 1, 5, 0, 1, 8, 2, 2, 6, 7, 1, 7, 7, 6, 1, 2, 4, 1, 7, 3, 1,\n",
       "       8, 4, 9, 4, 9, 7, 8, 5, 8, 4, 9, 2, 9, 2, 8, 7, 1, 4, 1, 4, 2, 1,\n",
       "       0, 0, 1, 8, 2, 2, 6, 6, 4, 9, 7, 4, 7, 5, 3, 4, 5, 7, 9, 5, 5, 9,\n",
       "       5, 3, 7, 3, 9, 6, 8, 2, 2, 7, 0, 4, 9, 8, 8, 8, 2, 6, 2, 8, 8, 0,\n",
       "       9, 9, 5, 6, 5, 1, 2, 0, 9, 7, 6, 5, 0, 1, 6, 7, 8, 1, 8, 6, 3, 3,\n",
       "       2, 4, 1, 1, 5, 0, 1, 8, 7, 3, 6, 7, 5, 2, 3, 6, 2, 6, 5, 5, 8, 4,\n",
       "       9, 0, 5, 6, 4, 0, 9, 7, 7, 5, 9, 7, 8, 4, 7, 9, 9, 8, 9, 2, 2],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "#print('Acur√°cia: ', f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def add_channels(array):\n",
    "\n",
    "    arr = np.expand_dims(array, axis = -1)\n",
    "    # pad the missing dims with zeros\n",
    "    pad = ((0,0),)*3 + ((0,2),)\n",
    "    padded = np.pad(arr, pad, 'constant', constant_values = 0)\n",
    "\n",
    "    padded = padded.reshape(padded.shape[0], padded.shape[1]*4, round(padded.shape[2]/4), 3)\n",
    "    \n",
    "\n",
    "    return padded\n",
    "\n",
    "X_train_mobile = add_channels(X_train)\n",
    "X_test_mobile = add_channels(X_test)\n",
    "\n",
    "IMG_SHAPE = (X_train_mobile.shape[1],X_train_mobile.shape[2] ,3)\n",
    "\n",
    "\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SHAPE),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
